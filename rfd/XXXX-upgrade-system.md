---
authors: Forrest Marshall (forrest@goteleport.com)
state: draft
---

# RFD XXXX - Upgrade System

## What

System for automatic upgrades of teleport installations.


## Why

Teleport must be perdiocially updated in order to integrate security patches.  Regular
updates also ensure that users can take advantage of improvements in stability and
performance.  Outdated teleport installations impose additional burdens on us and on
our users. Teleport does not currently assist with upgrades in any way, and the burden
of manual upgrades can be prohibitive.

Reducing the friction of upgrades is beneficial both in terms of security and user
experience. Doing this may also indirectly lower our own support load.


## Intro

### High-Level Goals

1. Maintain or improve the security of teleport installations by keeping them better
updated and potentially providing more secure paths to ugprade.

2. Improve the experience of teleport upgrade/administration by reducing the need for
manual intervention.

3. Improve the auditability of teleport clusters by providing insight into, and policy
enforcement for, the versioning of teleport installations.

4. Support a wide range of use-cases by providing a flexible and extensible set of tools
with support for things like user-provided upgrade scripts and custom target selection.

5. Offer a simple "batteries included" automatic upgrade option that requires minimal
configuration and "just works".


### Abstract Model Overview

This document proposes an modular system capable of supporting a wide range
of upgrade strategies, with the intention being that the default or "batteries
included" upgrade strategy will be implemented as a set of interchangeable
components, each of which could be swapped out for some alternative implementation.

The proposed system consists of at least the following components:

- `version-directive`: A static resource that describes the desired state of
versioning across the cluster. The directive includes matchers which allow
the auth server to match individual teleport instances with both the appropriate
installation target, and the appropriate installation method. This resource may
be periodically generated by teleport or by some custom external program. It may
also be manually created by an administrator.

- `version-controller`: A pluggable component responsible for generating the
`version-directive` based on some dynamic state (e.g. a server which publishes
package versions and hashes). A builtin `version-controller` would simply be a
configuration resource from the user's perspective. A custom/external version
controller would be any program with the permissions necessary to update the
`version-directive` resource.

- *version reconciliation loop*: A control loop that runs in the auth server which
comparse the desired state as specified by the `version-directive` with the current
state of teleport installations across the cluster. When mismatches are discovered,
the appropriate `install-controller`s are run.

- `install-controller`: A component capable of attempting to affect installation
of a specific target on one or more teleport hosts. The auth server needs to know
enough to at least start the process, but the core logic of a given `install-controller`
may be an external command (e.g. the proposed `local-script` controller would cause
each teleport instance in need of upgrade to run a user-supplied script locally and
then restart). From the perspective of a user, an `install-controller` is a teleport
configuration object (though that configuration object may only be a thin hook into
the "real" `install-controller`). Whether or not the teleport instance being upgraded
needs to understand the `install-controller` will depend on the specific controller.

There is room in the above model for additional granularity (e.g. schedulers), but
the listed components are a decent starting point.

### Implementation Plan Overview

Implementation will consist of four main phases:

1. Setup: Refactor our various service heartbeats into a unified bidirectional
GRPC stream. This will allow us to track the existence of teleport instances in a
consistent manner regardless of the specific services running on a given node.
It will also give us a mechanism for directly signaling individual nodes in order
to perform rolling upgrades. Improve teleport's self-knowledge, so that instances
can accurately report the specific build parameters that they correspond to
(arch/fips/etc) in addition to version. Make version inventory info available
via `tctl`.

2. MVP: Implement the `version-directive` resource, version reconciliation loop,
and `local-script` installer. This will serve both as a pathfinder/poc,
and also allow us to deliver a working means of easily performing cluster-wide
upgrades via existing upgrade mechanisms (e.g. `apt`/`yum`) in a much shorter time
than if we waited on the full "batteries included" feature being finished. This
iteration will only support non-auth instances, and remote rollbacks.

3. Batteries Included: Implement a `version-controller` and `install-controller`
based on [The Update Framework](https://theupdateframework.com/), and start
maintaining a TUF repository for teleport releases.  Implement a separate CI
pipeline for the TUF repository that can calculate package hashes using
deterministic compilation so that we have security isolation from mainline CI.
Extend auto-upgrade support to auth instances. Remote rollbacks only.

4. Bells And Whistles: Local rollbacks, pending/approval flow, client upgrades, etc...


## Details

### Usage Scenarios

TODO: update these sections with improved terminology.

#### Minimal Usecase

Cluster administrators explicitly set the desired version(s) of teleport installations, and
have a specific in-house installation script that should be used for upgrades. Teleport cluster
may not even have any access to the public internet.  The installation target may be an internally
built fork of teleport.

In this case, we want to provide the means to specify the target version and desired script. Teleport
should then be able to detect when a teleport node is not running the target that it aught to, and
invoke the upgrade script.  Teleport should not care about the internals of how the script plans to
perform the installation.  Nodes that require upgrades run the script, and may be required to perform
a graceful restart if the script succeeds. The script may expect inputs (e.g. version string), and
there may be different scripts to run depending on the nature of the specific node (e.g. `prod-upgrade`, `staging-upgrade`
or `apt-upgrade`, `yum-upgrade`), but things like selecting the correct processor architecture or OS
compatibility are likely handled by the script.

In this minimal usecase teleport's role is primarily as a coordinator.  It detects when and were user
provided scipts should be run, and invokes them.

#### Maximal Usecase

Cluster administrators opt into a mostly "set and forget" upgrade policy which keeps their teleport
cluster up to date automatically. They may wish to stay at a specific major version, but would like
to have patches and minor backwards-compatible improvements come in automatically.  They want features
like maintenance schedules to prevent a node from being upgraded when they need it available, and automatic
rollbacks where nodes revert to their previous installation version of they are unhealthy for too long.
They also want teleport to be able to upgrade itself without dependencies.

This usecase requires the same coordination powers as the minimal usecase, but also a lot more.  Teleport
needs to be able to securely and reliably detect new releases when they become availalbe.  Teleport needs
to be able to evaluate new releases in the context of flexible upgrade policies and select which releases
(and which targets within those releases) are appropriate and when they should be installed. Teleport needs
to be able to download and verify installation packages, upgrade itself, and monitor the health of newly
installed versions.

In this maximal usecase, teleport is responsible for discovery, selection, coordination, validation, and
monitoring.  Most importantly, teleport must do all of this in a secure and reliable manner.  The potential
fallout from bugs and vulnerabilities is greater than in the minimal usecase.

#### Hacker Usecase

Cluster administrators march to the beat of their own drum.  They want to know the latest publically available
teleport releases, but skip all prime number patches.  Nodes can only be upgraded if it is low
tide in their region, the moon is waxing, and the ISS is on the other side of the planet. They want to use
teleport's native download and verification logic, but they also need to start the downloaded binary in
a sandbox first to ensure it won't trigger their server's self-destruct.  If rollback is necessary, the rollback
reason and timestamp need to be steganographically encoded into a picture of a turtle and posted to instagram.

This usecase has essentially the same requirements as the maximual usecase, with one addition.  It necessitates
*loose coupling* of componentes.


### Security

#### General

- The `version-directive` must specify concrete version numbers, and provide any metadata necessary for
package validation (varies by install method).  Any automated system which generates `version-directive`s
(e.g. a TUF `version-controller` or some external version control plugin) must not fail silently (may indicate a censorship attack).
Teleport can assist with this by emitting warnings if the directive is outdated.

- Teleport should detect and warn about invalid state-transitions proposed by a `version-directive`, and
refuse to enact them (e.g. refusing to downgrade from `v3.4.5` to `v1.2.3`). These checks should be independent
of a specific controller (i.e. they should apply to custom plugins too).

- We should encourage decentralized trust. The TUF based system should leverage TUF's multisignature support
to ensure that compromise of a single key cannot compromise installations. We should also provide tools
to help those using custom installation mechanism to avoid single-point failures as well (e.g. mutliparty approval 
for pending `version-directive`s).

- TODO


#### TUF-Specific Security Model

- TODO: explain TUF security model, thresholded signing + deterministic compilation, `teleport verify`,
fips compat, etc.


### Inventory Status and Control Model

- Auth servers exert direct control over non-auth instance upgrades via bidirectional
GRPC control stream.

- Non-auth instances advertise detailed information about the current isntallation, and
implement handlers for control stream messages that can execute whatever local component
is required for running a given install method (e.g. executing a specific script if the
`local-script` installer is in use).

- Each control stream is registered with a single auth server, so each auth server is
responsible for triggering the upgrade of a subset of the server inventory.  In order
to reduce thundering herd effects, upgrades will be rolling with some reasonable default
rate.

- Upgrade decisisons are level-based. Remote downgrades and retries are an emergent
property of a level-based system, and won't be given special treatment

- The auth server may skip a directive that it recognizes as resulting in an incompatible
change in version (e.g. skipping a full major version).

- By default, non-standard installations are not upgraded (e.g. `1.2.3-debug.2`).

- In order to avoid nearly doubling the amount of backend writes for existing large
clusters (all of whose instances are predominantly ssh services), the existing "node"
resource (which would be more accurately described as the `ssh_server` resource), will
be repurposed to represent a server installation which may or may not be running an ssh
service.  Whether or not other services would also benefit from unification in this way
can be evaluated on a case-by-case basis down the road.

- TODO: protobuf spec for control stream and installation spec.


### Version Directive and Reconciliation Loop

- Nonce and controller of origin are enforced to prevent concurrent or unordered updates to
version directive.

- Selectors, targets, and installers are defined as ordered sequences. Reconciliation loop always
halts on first matching/compatible item in a sequence.

- Reconciliation loop high-level steps:
  - Load `version-directive` resource.
  - Match connected servers to directives via selector.
  - Match targets from appropriate directives to servers based on currently installed target.
  - Filter out servers which already have the correct target installed.
  - Pass remaining servers to first compatible install controller.

TODO

```yaml
# version directive is a singleton resource that is either supplied by a user,
# or periodically generated by a version controller (e.g. tuf, plugin, etc).
# this represents the desired state of the cluster, and is used to guide a control
# loop that matches install targets to appropriate nodes and installers.
kind: version-directive
version: v1
metadata:
  name: version-directive
spec:
  nonce: 2
  status: enabled
  version_controller: static-config
  not_before: <time>
  not_after: <time>
  directives:
    - name: Staging
      targets:
        - version: 2.3.4
          fips: yes
        - version: 2.3.4
          fips: no
      installers:
        - kind: script
          name: apt-install
      selectors:
        - labels:
            env: staging
        - labels:
            env: testing

    - name: Prod
      targets:
        - version: 1.2.3
          fips: yes
      installers:
        - kind: script
          name: apt-install
      selectors:
        - labels:
            env: prod

    - name: Minimum
      targets:
        - version: 1.2.0
          fips: no
      installers:
        kind: script
        name: apt-install
      selectors:
        - labels:
            '*': '*'
```


### Version Controllers

- A `version-controller` is a loop that periodically generates a `version-directive`.  It may
be a loop that runs within the auth server, or an external plugin.

- The only currently planned `version-controller` is the TUF version controller.


#### TUF Version Controller

- The TUF version controller will be based on [go-tuf](https://github.com/theupdateframework/go-tuf)
and will maintain TUF client state within the teleport backend (TUF clients are statful, since they
need to support concepts like key rotation).

- When enabled, the TUF controller will periodically sync with a TUF repository that we maintain.  It
will download the evailable pakckages and use its `directive_generators` to generate a `version-directive`
based on the available packages.

- TODO: fips mode


```yaml
# a version controller runs on a control loop, gathering information about
# available installation targets and updating the version-directive resource.
# a version controller with `status: pending` actively generates version
# directives, but emits them to a pending location instead.  this pattern
# allows the injection of user-controlled logic, where a user may write a
# script that gets the pending directive, modifies it, then pushes it
# to the active slot.
kind: version-controller
version: v1
sub_kind: tuf
metadata:
  name: tuf-controller
spec:
    status: enabled
    directive_generators:
      - name: Staging
        target_selectors:
          - channel: stable
            version: 7.*
        server_selectors:
          - labels:
              env: staging
      - name: Prod
        target_selectors:
          - channel: stable
            version: 7.2.*
        server_selectors:
          - labels:
              env: prod
      - name: Minimum
        target_selectors:
          - channel: stable
            version: 6.*
        server_selectors:
          - labels:
              '*': '*'
```


### Install Controllers

- From the point of view of the version reconciliation loop, an install controller is essentially
a function of the form `install(server_spec, target_spec)`.

- TODO: High-level overview of the `install-controller` concept.


#### Local-Script Install Controller

- The `local-script` installer runs the provided script on the host that is in need of upgrade.

- The installer accepts the installation `target` as an "arguemnt", so that the version and any
other relevant metadata can be extracted.  If the `target` is missing any of the expected metadata,
the installer fails (different version controllers may generate different metadata).

- 

```yaml
# an installer attempts to apply an installation target to a node. this is an example
# of an installer that gets passed from the auth server to the node so that the node
# itself can run it, but some installers may run somewhere other than the node itself
# (e.g. if invoking some API that remotely upgrades teleport installs). The auth server
# uses the version-directive to determine which installers should be run for which nodes
# and with which targets.
kind: installer
sub_kind: script
version: v1
metadata:
  name: apt-install
spec:
  enabled: yes
  env:
      "VERSION": '{target.version}'
  shell: /bin/bash
  script: |
    set -euo pipefail
    apt install teleport-${VERSION:?}
```

#### TUF Install Controller

- The TUF install controller will not need to be configured by users.  It will be the
default intall controller used whenever the TUF `version-controller` is active.

- The TUF install controller will download the appropriate package from `get.gravitational.com`
and perform standard TUF veridication (hash + size).

#### `teleport verify`

- In order to support more advanced usecases, the process of verifying a package against its associated
TUF metadata will be exposed as a new `teleport verify` command.  This will allow users that need
custom install logic to use TUF metadata from the TUF version controller to verify their packages.

- In the interest of cross-version compatibility, the TUF metadata will be an arbitrary blob from the
point of view of the script installer.


### TUF CI and Repository

In order to enable the TUF version controller, we will need to maintain CI that generates and signs
TUF metadata, and maintain a TUF repository.  Details of how the TUF respository will be hosted are
still TBD, but TUF repositories are basically static files, so distribution should be fairly straightforward.
We may be able to simply distribute it via a git repo.

We will leverage deterministic builds and TUF's multisignature support to harden ourselves against CI
compromise. Our standard build pipeline will generate and sign one set of package hashes, and another set
will be generated and signed by a separate isolated env.

TUF repositores prove liveness via periodic resigning with a "hot" key (not the keys used for package signing).
This hot key should be isolated from the package signing keys, so we're likely looking at two new isolated
envs that need to be added in addition to the modifications to our existing CI.

- NOTE: some initial work was done to get deterministic builds working on linux packages. We know its possible
(and might even still be working), but don't currently have test coverage for build determinism. This will be
an important part of the prerequisite work to get the TUF system online. We don't neeed to add TUF support for
all build targets at once, so we may specifically target reliable signing of amd64/linux packages first.

 
## Notes

### Open Questions

- How important is establishing a workflow around pending/approval (e.g. have automatic disovery and
automatic installation, but wait for administrative intervention before actually updating the directive)?
We want this eventually, but is it of sufficient priority to be included from the beginning?

- How best to support hybrid/mapped `version-controller`s? (e.g. if a user wants to apply some custom
modification to the output of the TUF controller to better suit their specific needs) Should it be
possible to emit a pending directive that gets modified and placed in the active slot by a separate
task?  Should an explicit mapping/modifier component be defined?

- How should client upgrades work? Having clients pull their source of truth from the teleport cluster
is potentially confusing in multi-cluster environments.  Maybe a TUF-based `tsh upgrade` command?  Do
we care about supporting alternative/pluggable install for clients?

- How should local-rollbacks work? Presumably some background process will fork and monitor the new
instance. What should be the default affect if the background process itself fails?  E.g. are we
completing the new install prior to launching it, or are we launching the new instance in some kind
of pending state and only finalizing the install if it appears healthy?

- Figure out a good model for providing visibility into current upgrade/versioning status.

- Rename `version-controller` to `manifest-controller`? Names are hard.
